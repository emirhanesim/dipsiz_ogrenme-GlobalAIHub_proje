{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage import transform\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import ndarray\n",
    "from sklearn.model_selection import train_test_split \n",
    "import torch.utils.data as data_utils\n",
    "import torchvision\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>99812-1-2-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>159.522205</td>\n",
       "      <td>163.522205</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>99812-1-3-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>181.142431</td>\n",
       "      <td>183.284976</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>99812-1-4-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>242.691902</td>\n",
       "      <td>246.197885</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>99812-1-5-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>253.209850</td>\n",
       "      <td>255.741948</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>99812-1-6-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>332.289233</td>\n",
       "      <td>334.821332</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name   fsID       start         end  salience  fold  classID  \\\n",
       "8727  99812-1-2-0.wav  99812  159.522205  163.522205         2     7        1   \n",
       "8728  99812-1-3-0.wav  99812  181.142431  183.284976         2     7        1   \n",
       "8729  99812-1-4-0.wav  99812  242.691902  246.197885         2     7        1   \n",
       "8730  99812-1-5-0.wav  99812  253.209850  255.741948         2     7        1   \n",
       "8731  99812-1-6-0.wav  99812  332.289233  334.821332         2     7        1   \n",
       "\n",
       "         class  \n",
       "8727  car_horn  \n",
       "8728  car_horn  \n",
       "8729  car_horn  \n",
       "8730  car_horn  \n",
       "8731  car_horn  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#veri bilgilerinin bulunduğu csv dosyasını okuma\n",
    "csv_path = r\"C:\\Users\\emirh\\Desktop\\ai_hub_proje\\data\\UrbanSound8K\\metadata\\UrbanSound8K.csv\"\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "csv_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#görüntünün veri yolunu input olarak alır ve sırasıyla;\n",
    "#grayscale dönüşümü, resizing, normalization, [görüntü, etiket] formatına yazma\n",
    "#işlemlerini yapar.\n",
    "#[görüntü,etiket] formatında DataFrame'i oluşturur ve csv dosyası olarak kaydını yapar \n",
    "\n",
    "\n",
    "def preprocessor(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for foldername in os.listdir(directory): #spectrograms klasöründeki classID pathlerini döndür\n",
    "        f = os.path.join(directory, foldername) #classID klasörlerinin pathleri\n",
    "        for imagename in os.listdir(f):\n",
    "            imagepath = os.path.join(f, imagename)\n",
    "            rgb_img = io.imread(imagepath)[:,:,:3]\n",
    "            gray_img = color.rgb2gray(rgb_img)\n",
    "            gray_img_resized = transform.resize(gray_img, (128,128)) #grayscaled fotoğraf\n",
    "            image = np.array(gray_img_resized, 'float32')\n",
    "            image = (image-image.mean())/image.std()\n",
    "            label = int(foldername)    \n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'C:\\Users\\emirh\\Desktop\\ai_hub_proje\\data\\spectrograms'\n",
    "\n",
    "images, labels = preprocessor(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 8732)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.6713247 , -0.28672266, -1.8196371 , ..., -1.8227298 ,\n",
       "        -1.8227298 , -1.8227298 ],\n",
       "       [ 3.677114  , -0.27035457, -1.8173009 , ..., -1.8204195 ,\n",
       "        -1.8204175 , -1.8204162 ],\n",
       "       [ 3.6771352 , -0.27029043, -1.8172846 , ..., -1.8203738 ,\n",
       "        -1.8129799 , -1.8074372 ],\n",
       "       ...,\n",
       "       [ 3.6768723 , -0.20892416, -0.86748713, ...,  1.884726  ,\n",
       "         1.6924425 ,  0.9927972 ],\n",
       "       [ 3.7136507 ,  1.0344989 ,  0.2612872 , ...,  0.9487419 ,\n",
       "         0.8893153 ,  0.67563593],\n",
       "       [ 3.8196363 ,  3.7934604 ,  3.7889116 , ...,  3.7891667 ,\n",
       "         3.7891512 ,  3.7890959 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.6713247 , -0.28672266, -1.8196371 , ..., -1.8227298 ,\n",
       "        -1.8227298 , -1.8227298 ],\n",
       "       [ 3.677114  , -0.27035457, -1.8173009 , ..., -1.8204195 ,\n",
       "        -1.8204175 , -1.8204162 ],\n",
       "       [ 3.6771352 , -0.27029043, -1.8172846 , ..., -1.8203738 ,\n",
       "        -1.8129799 , -1.8074372 ],\n",
       "       ...,\n",
       "       [ 3.6768723 , -0.20892416, -0.86748713, ...,  1.884726  ,\n",
       "         1.6924425 ,  0.9927972 ],\n",
       "       [ 3.7136507 ,  1.0344989 ,  0.2612872 , ...,  0.9487419 ,\n",
       "         0.8893153 ,  0.67563593],\n",
       "       [ 3.8196363 ,  3.7934604 ,  3.7889116 , ...,  3.7891667 ,\n",
       "         3.7891512 ,  3.7890959 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = np.array(X_train)\n",
    "X_test_np = np.array(X_test)\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6985, 128, 128)\n",
      "(1747, 128, 128)\n",
      "(6985,)\n",
      "(1747,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_np.shape)\n",
    "print(X_test_np.shape)\n",
    "print(y_train_np.shape)\n",
    "print(y_test_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.from_numpy(X_train_np).unsqueeze(-1).permute(0,3,1,2)\n",
    "X_test_t = torch.from_numpy(X_test_np).unsqueeze(-1).permute(0,3,1,2)\n",
    "y_train_t = torch.from_numpy(y_train_np)\n",
    "y_test_t = torch.from_numpy(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6985, 1, 128, 128]), torch.Size([1747, 1, 128, 128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t.shape, X_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6985]), torch.Size([1747]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_t.shape, y_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_utils.TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)\n",
    "test = data_utils.TensorDataset(X_test_t, y_test_t)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN modelini tanımlama\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(in_features=128*9*9, out_features=10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function ve optimizer tanımla\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2/400, loss=1.7524\n",
      "epoch4/400, loss=1.7048\n",
      "epoch6/400, loss=1.6566\n",
      "epoch8/400, loss=1.7315\n",
      "epoch10/400, loss=1.7131\n",
      "epoch12/400, loss=1.6800\n",
      "epoch14/400, loss=1.6791\n",
      "epoch16/400, loss=1.6526\n",
      "epoch18/400, loss=1.5708\n",
      "epoch20/400, loss=1.6140\n",
      "epoch22/400, loss=1.6547\n",
      "epoch24/400, loss=1.5988\n",
      "epoch26/400, loss=1.5306\n",
      "epoch28/400, loss=1.5843\n",
      "epoch30/400, loss=1.6429\n",
      "epoch32/400, loss=1.5588\n",
      "epoch34/400, loss=1.5570\n",
      "epoch36/400, loss=1.6382\n",
      "epoch38/400, loss=1.5979\n",
      "epoch40/400, loss=1.5681\n",
      "epoch42/400, loss=1.5576\n",
      "epoch44/400, loss=1.6247\n",
      "epoch46/400, loss=1.5303\n",
      "epoch48/400, loss=1.7082\n",
      "epoch50/400, loss=1.5726\n",
      "epoch52/400, loss=1.5563\n",
      "epoch54/400, loss=1.5436\n",
      "epoch56/400, loss=1.6296\n",
      "epoch58/400, loss=1.5427\n",
      "epoch60/400, loss=1.6251\n",
      "epoch62/400, loss=1.5701\n",
      "epoch64/400, loss=1.6115\n",
      "epoch66/400, loss=1.6239\n",
      "epoch68/400, loss=1.5703\n",
      "epoch70/400, loss=1.6372\n",
      "epoch72/400, loss=1.6252\n",
      "epoch74/400, loss=1.5297\n",
      "epoch76/400, loss=1.6253\n",
      "epoch78/400, loss=1.5692\n",
      "epoch80/400, loss=1.5564\n",
      "epoch82/400, loss=1.5421\n",
      "epoch84/400, loss=1.6219\n",
      "epoch86/400, loss=1.5568\n",
      "epoch88/400, loss=1.6105\n",
      "epoch90/400, loss=1.6115\n",
      "epoch92/400, loss=1.6455\n",
      "epoch94/400, loss=1.5566\n",
      "epoch96/400, loss=1.5701\n",
      "epoch98/400, loss=1.6796\n",
      "epoch100/400, loss=1.6489\n",
      "epoch102/400, loss=1.5841\n",
      "epoch104/400, loss=1.5973\n",
      "epoch106/400, loss=1.5427\n",
      "epoch108/400, loss=1.6271\n",
      "epoch110/400, loss=1.5837\n",
      "epoch112/400, loss=1.6486\n",
      "epoch114/400, loss=1.6019\n",
      "epoch116/400, loss=1.5861\n",
      "epoch118/400, loss=1.5265\n",
      "epoch120/400, loss=1.5159\n",
      "epoch122/400, loss=1.5452\n",
      "epoch124/400, loss=1.5434\n",
      "epoch126/400, loss=1.5842\n",
      "epoch128/400, loss=1.5160\n",
      "epoch130/400, loss=1.5706\n",
      "epoch132/400, loss=1.5707\n",
      "epoch134/400, loss=1.5296\n",
      "epoch136/400, loss=1.5697\n",
      "epoch138/400, loss=1.5571\n",
      "epoch140/400, loss=1.5296\n",
      "epoch142/400, loss=1.5023\n",
      "epoch144/400, loss=1.6001\n",
      "epoch146/400, loss=1.5293\n",
      "epoch148/400, loss=1.4882\n",
      "epoch150/400, loss=1.5844\n",
      "epoch152/400, loss=1.5448\n",
      "epoch154/400, loss=1.6116\n",
      "epoch156/400, loss=1.5570\n",
      "epoch158/400, loss=1.4885\n",
      "epoch160/400, loss=1.5335\n",
      "epoch162/400, loss=1.5092\n",
      "epoch164/400, loss=1.5163\n",
      "epoch166/400, loss=1.5433\n",
      "epoch168/400, loss=1.5299\n",
      "epoch170/400, loss=1.6116\n",
      "epoch172/400, loss=1.5570\n",
      "epoch174/400, loss=1.5567\n",
      "epoch176/400, loss=1.5296\n",
      "epoch178/400, loss=1.5433\n",
      "epoch180/400, loss=1.4997\n",
      "epoch182/400, loss=1.5351\n",
      "epoch184/400, loss=1.5660\n",
      "epoch186/400, loss=1.5570\n",
      "epoch188/400, loss=1.5430\n",
      "epoch190/400, loss=1.5981\n",
      "epoch192/400, loss=1.5746\n",
      "epoch194/400, loss=1.5391\n",
      "epoch196/400, loss=1.5570\n",
      "epoch198/400, loss=1.5620\n",
      "epoch200/400, loss=1.5578\n",
      "epoch202/400, loss=1.5570\n",
      "epoch204/400, loss=1.5292\n",
      "epoch206/400, loss=1.5434\n",
      "epoch208/400, loss=1.5980\n",
      "epoch210/400, loss=1.5843\n",
      "epoch212/400, loss=1.5022\n",
      "epoch214/400, loss=1.6119\n",
      "epoch216/400, loss=1.5981\n",
      "epoch218/400, loss=1.5707\n",
      "epoch220/400, loss=1.5022\n",
      "epoch222/400, loss=1.5189\n",
      "epoch224/400, loss=1.5296\n",
      "epoch226/400, loss=1.5159\n",
      "epoch228/400, loss=1.5568\n",
      "epoch230/400, loss=1.5159\n",
      "epoch232/400, loss=1.5159\n",
      "epoch234/400, loss=1.5981\n",
      "epoch236/400, loss=1.5433\n",
      "epoch238/400, loss=1.5022\n",
      "epoch240/400, loss=1.5457\n",
      "epoch242/400, loss=1.5296\n",
      "epoch244/400, loss=1.5433\n",
      "epoch246/400, loss=1.5158\n",
      "epoch248/400, loss=1.5159\n",
      "epoch250/400, loss=1.5304\n",
      "epoch252/400, loss=1.5570\n",
      "epoch254/400, loss=1.5066\n",
      "epoch256/400, loss=1.5157\n",
      "epoch258/400, loss=1.5159\n",
      "epoch260/400, loss=1.4881\n",
      "epoch262/400, loss=1.5159\n",
      "epoch264/400, loss=1.5837\n",
      "epoch266/400, loss=1.5841\n",
      "epoch268/400, loss=1.5296\n",
      "epoch270/400, loss=1.6255\n",
      "epoch272/400, loss=1.5401\n",
      "epoch274/400, loss=1.5570\n",
      "epoch276/400, loss=1.5981\n",
      "epoch278/400, loss=1.5159\n",
      "epoch280/400, loss=1.5433\n",
      "epoch282/400, loss=1.5707\n",
      "epoch284/400, loss=1.5844\n",
      "epoch286/400, loss=1.5179\n",
      "epoch288/400, loss=1.5632\n",
      "epoch290/400, loss=1.5707\n",
      "epoch292/400, loss=1.5570\n",
      "epoch294/400, loss=1.5566\n",
      "epoch296/400, loss=1.6118\n",
      "epoch298/400, loss=1.5377\n",
      "epoch300/400, loss=1.5702\n",
      "epoch302/400, loss=1.5159\n",
      "epoch304/400, loss=1.5296\n",
      "epoch306/400, loss=1.5433\n",
      "epoch308/400, loss=1.5022\n",
      "epoch310/400, loss=1.5707\n",
      "epoch312/400, loss=1.5568\n",
      "epoch314/400, loss=1.5433\n",
      "epoch316/400, loss=1.5981\n",
      "epoch318/400, loss=1.4748\n",
      "epoch320/400, loss=1.5159\n",
      "epoch322/400, loss=1.5022\n",
      "epoch324/400, loss=1.5054\n",
      "epoch326/400, loss=1.5570\n",
      "epoch328/400, loss=1.5566\n",
      "epoch330/400, loss=1.5707\n",
      "epoch332/400, loss=1.5570\n",
      "epoch334/400, loss=1.5433\n",
      "epoch336/400, loss=1.5433\n",
      "epoch338/400, loss=1.5844\n",
      "epoch340/400, loss=1.4748\n",
      "epoch342/400, loss=1.5239\n",
      "epoch344/400, loss=1.5296\n",
      "epoch346/400, loss=1.5845\n",
      "epoch348/400, loss=1.5981\n",
      "epoch350/400, loss=1.5429\n",
      "epoch352/400, loss=1.5022\n",
      "epoch354/400, loss=1.4885\n",
      "epoch356/400, loss=1.5570\n",
      "epoch358/400, loss=1.5552\n",
      "epoch360/400, loss=1.5968\n",
      "epoch362/400, loss=1.5159\n",
      "epoch364/400, loss=1.5161\n",
      "epoch366/400, loss=1.5433\n",
      "epoch368/400, loss=1.5022\n",
      "epoch370/400, loss=1.5707\n",
      "epoch372/400, loss=1.5293\n",
      "epoch374/400, loss=1.5433\n",
      "epoch376/400, loss=1.5844\n",
      "epoch378/400, loss=1.5296\n",
      "epoch380/400, loss=1.5159\n",
      "epoch382/400, loss=1.5022\n",
      "epoch384/400, loss=1.5297\n",
      "epoch386/400, loss=1.5022\n",
      "epoch388/400, loss=1.5981\n",
      "epoch390/400, loss=1.5159\n",
      "epoch392/400, loss=1.5159\n",
      "epoch394/400, loss=1.5707\n",
      "epoch396/400, loss=1.5707\n",
      "epoch398/400, loss=1.5705\n",
      "epoch400/400, loss=1.5570\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Modelin eğitimi\n",
    "epochs = 400\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        inputs, labelss = data\n",
    "        labelss = labelss.type(torch.LongTensor)\n",
    "        inputs = inputs.to(device)\n",
    "        labelss = labelss.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labelss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'epoch{epoch+1}/{epochs}, loss={loss.item():.4f}')    \n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\emirh\\Desktop\\ai_hub_proje\\urbansounds_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  drilling // jackhammer // siren // siren // engine_idling // car_horn // drilling // siren // siren // car_horn\n"
     ]
    }
   ],
   "source": [
    "#modelin test edilmesi\n",
    "classes = [\"air_conditioner\", \"car_horn\", \"children_playing\", \"dog_bark\", \"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\", \"siren\", \"street_music\"]\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "print('GroundTruth: ', ' // '.join(f'{classes[labels[j]]:5s}' for j in range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  drilling // jackhammer // siren // siren // engine_idling // street_music // drilling // siren // siren // street_music\n"
     ]
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' // '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: air_conditioner is 83.6 %\n",
      "Accuracy for class: car_horn is 0.0 %\n",
      "Accuracy for class: children_playing is 80.7 %\n",
      "Accuracy for class: dog_bark is 86.0 %\n",
      "Accuracy for class: drilling is 92.4 %\n",
      "Accuracy for class: engine_idling is 86.0 %\n",
      "Accuracy for class: gun_shot is 96.1 %\n",
      "Accuracy for class: jackhammer is 94.8 %\n",
      "Accuracy for class: siren is 89.7 %\n",
      "Accuracy for class: street_music is 80.6 %\n"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02f0e89a1197599f3dc7c16f0e4f47d78f3d8e79d75528e8a24eade9b59b02bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
